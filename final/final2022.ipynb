{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《深度学习》2022-2023学年第1学期期末考试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 姓名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示与建议：\n",
    "\n",
    "1. 优先完成更有把握的题目；\n",
    "2. 对于简答题，回答核心要点即可，不要提供冗长的陈述；\n",
    "3. 对于编程题，优先完成整体的框架，有时间富余时再进行细节上的调试；\n",
    "4. 对于综合题，每一小问的题目本身可能就是对后续问题的提示；\n",
    "5. 综合题小问的难度不一定是逐渐上升的，先把题目读完，优先完成有把握的问题；\n",
    "6. 可参考课件及以往的作业，但注意与考题的差异，不要生搬硬套。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题：简答题（20分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) 某信用卡公司希望建立一个深度学习模型，通过分析申请者的背景信息来决定是否发放信用卡。如果使用一个前馈神经网络来建立模型，则该神经网络的输出层应该使用什么激活函数（包括不使用激活函数）？为什么？当该公司决定为某些申请者发放信用卡后，希望再建立一个神经网络，用来决定用户的最大借款额度，那么该神经网络的输出层应该使用什么激活函数（包括不使用激活函数）？为什么？（10分）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）是否发放用sigmoid函数，因为sigmoid的导数在0附近的时候有比较好的激活性，并且y轴取值范围[0，1]，得到的值都是正的，这些都是二分类问题需要的。\n",
    "\n",
    "对于二分类问题，如判断是否发放信用卡，通常会选择在输出层使用 Sigmoid 激活函数。Sigmoid 函数将网络的原始输出映射到 [0, 1] 范围内，可以看作是一个概率值，表示正类别的概率。模型的输出可以被解释为申请者获得信用卡的概率。因此，输出层的激活函数可以选择 Sigmoid。\n",
    "\n",
    "（2）最大借款额度神经网络的输出层用ReLU函数，因为用户的最大借款额度必≥0，ReLU函数的输出范围是[0,+∞]，且ReLU在＞0的区域永远有梯度，且计算简单，便于求解用户的最大借款额度。\n",
    "\n",
    "对于回归问题，如决定用户的最大借款额度，通常不使用激活函数或者使用线性激活函数。这是因为在回归问题中，我们希望网络的输出直接表示预测值，而不需要进行概率映射。线性激活函数允许输出取任意实数值，不做范围限制，适合表示连续的数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) 在(a)问中，建立好两个模型之后，各自可以采用什么损失函数来进行模型训练？简要叙述其理由。（5分）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二分类问题（是否发放信用卡），通常会选择二元交叉熵损失函数（Binary Cross Entropy Loss）。该损失函数对于二分类问题是一个合适的选择，尤其是在使用 Sigmoid 激活函数作为输出层的情况下。Binary Cross Entropy Loss 在测量两个概率分布之间的距离时非常有效，它使得模型的输出概率分布与真实标签的分布越接近，损失越小。\n",
    "\n",
    "对于回归问题（用户的最大借款额度），常用的损失函数包括均方误差损失函数（Mean Squared Error，MSE）。MSE 适用于回归问题，它度量模型输出与真实值之间的平均平方差，越小表示模型预测越准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) 在一个典型的前馈神经网络中，每一层的隐藏神经元都要先经过一个激活函数后再传递给下一层。是否可以让某些层不经过激活而直接传递给下一层？为什么？（5分）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不可以。相邻的两层都是是线性操作就能够合并成为一个线性操作，激活函数的目的是让数据呈现非线性，有了非线性的激活，就不能合并了。\n",
    "\n",
    "在典型的前馈神经网络中，每一层的隐藏神经元通常都会经过一个激活函数。激活函数的作用是引入非线性变换，使得神经网络可以学习复杂的非线性关系。如果某些层不经过激活函数而直接传递给下一层，相当于该层只进行线性变换，失去了引入非线性的作用。\n",
    "\n",
    "在神经网络中，线性变换的堆叠等效于单一的线性变换。如果没有非线性激活函数，整个网络将无法表示复杂的非线性关系，限制了模型的表达能力。激活函数的引入允许网络学习非线性模式，使得神经网络可以逼近任意复杂的函数。\n",
    "\n",
    "因此，一般来说，为了提高神经网络的表达能力，隐藏层中的神经元通常都要经过激活函数。当然，具体的网络结构和问题要求可能会有例外，但在大多数情况下，引入非线性激活函数是神经网络学习非线性关系的关键。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题：编程题（45分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) 利用 PyTorch 编写一个函数，计算 $d$ 维标准正态分布的对数密度函数 $\\log[p_Z(z)]$，其中**\n",
    "\n",
    "$$p_{Z}(z)=(2\\pi)^{-d/2}\\exp\\left(-\\frac{1}{2}\\Vert z\\Vert^{2}\\right).$$\n",
    "\n",
    "**该函数的输入 `z` 是一个 $n\\times d$ 的矩阵，输出 `l` 是一个 $n\\times 1$ 的向量，其中 `l` 的第 $i$ 个元素是 $\\log[p_Z(\\cdot)]$ 在 `z` 的第 $i$ 行上的取值。（5分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4305, -0.3499,  0.4749,  0.9041, -0.7021,  1.5963],\n",
      "        [ 0.4228, -0.6940,  0.9672,  1.5569, -2.3860,  0.6994],\n",
      "        [-1.0325, -2.6043,  0.8177,  0.1459, -0.9558,  1.4745],\n",
      "        [ 0.5109, -0.2325,  0.3958,  0.8536,  0.8275, -1.0542],\n",
      "        [-0.7374, -0.4202,  0.3071, -1.2767,  0.2009,  0.0190]])\n",
      "tensor([ -7.7096, -10.6147, -11.3267,  -7.0118,  -6.7563])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def log_normal_pdf(z):\n",
    "    # 在此处完成函数实现\n",
    "    d = z.shape[1]\n",
    "    p_z = (2 * math.pi)**(-d/2) * torch.exp(-1/2 * torch.sum(z * z, dim=1)) \n",
    "    # e = torch.linalg.norm(z,ord=2,axis=1) # e = e**2\n",
    "    l = torch.log(p_z)\n",
    "    return l\n",
    "\n",
    "# 测试结果\n",
    "torch.manual_seed(2023)\n",
    "z = torch.randn(5, 6)\n",
    "\n",
    "print(z)\n",
    "print(log_normal_pdf(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) 利用 PyTorch 编写一个函数 `log_add_exp(x, y)`，其接收两个大小相同的 Tensor 作为输入（x 和 y），并逐元素计算 $f(x,y)=\\log(e^x+e^y)$，最后返回一个同样大小的 Tensor。请注意计算的数值稳定性，并在如下数据上测试计算结果。PyTorch 中提供了 `torch.logaddexp()` 函数用来实现该运算，你可以使用它验证你的结果，但不能直接调用它。（10分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.9931e+02, -9.3069e+00,  6.9315e-01,  1.0693e+01,  1.0069e+02,\n",
      "         1.0000e+02,  1.0000e+01,  6.9315e-01,  1.0000e+01,  1.0000e+03])\n",
      "tensor([-9.9931e+02, -9.3069e+00,  6.9315e-01,  1.0693e+01,  1.0069e+02,\n",
      "         1.0000e+02,  1.0000e+01,  6.9315e-01,  1.0000e+01,  1.0000e+03])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def log_add_exp(x, y):\n",
    "    # 在此处完成函数实现\n",
    "    max_values = torch.max(x, y)\n",
    "    result = max_values + torch.log1p(torch.exp(-torch.abs(x - y)))\n",
    "    return result\n",
    "\n",
    "# 测试结果\n",
    "x = torch.tensor([-1000.0, -10.0, 0.0, 10.0, 100.0, -100.0, -10.0, 0.0, 10.0, 1000.0])\n",
    "y = torch.tensor([-1000.0, -10.0, 0.0, 10.0, 100.0, 100.0, 10.0, 0.0, -10.0, -1000.0])\n",
    "\n",
    "print(log_add_exp(x, y))\n",
    "print(torch.logaddexp(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) 给定矩阵 $X=(x_{ij})$ 和函数 $f(X)=\\log|\\det(Y'Y)|$，其中 $Y=(y_{ij})$, $y_{ij}=\\log(10+\\exp(x_{ij}))$，$\\det(A)$ 表示方阵 $A$ 的行列式，$|\\det(A)|$ 是 $\\det(A)$ 的绝对值。编写 PyTorch 程序，计算 $f(X)$ 关于 $X$ 的导数在 $X=X_0$ 处的取值，其中 $X_0$ 由如下代码给出。（10分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.1308e-04, -1.7771e-19,  2.2686e-03,  3.7617e-03, -4.2362e-35],\n",
       "        [ 7.0432e-04, -7.6684e-03,  1.7234e-34, -1.0274e-04,  1.2052e-02],\n",
       "        [ 0.0000e+00,  6.0710e-03,  0.0000e+00,  0.0000e+00,  4.8538e-03],\n",
       "        [ 1.9263e-09,  1.7671e-02,  0.0000e+00,  3.3120e-22, -7.0877e-03],\n",
       "        [ 0.0000e+00,  7.7662e-03, -7.0065e-45,  1.5058e-32, -2.1457e-07],\n",
       "        [ 0.0000e+00,  0.0000e+00, -4.4620e-04,  5.8816e-03, -7.0065e-45],\n",
       "        [-5.9144e-04,  5.1088e-03,  0.0000e+00,  6.3688e-03, -3.8515e-03],\n",
       "        [ 7.4336e-03,  5.7173e-43,  0.0000e+00, -1.8102e-14,  0.0000e+00],\n",
       "        [ 3.7487e-04,  6.2881e-03,  2.7540e-03, -2.2399e-21, -7.3817e-33],\n",
       "        [-3.2028e-15, -4.2453e-03,  8.2968e-03,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2023)\n",
    "X0 = 100.0 * torch.randn(10, 5)\n",
    "\n",
    "# 在此处完成代码实现\n",
    "X0.requires_grad = True\n",
    "def softplus_10(x):\n",
    "    # 完成函数体\n",
    "    e = torch.exp(-torch.abs(x)) # e^(-x)\n",
    "    s_x = torch.where(x >= 0, x + torch.log(1 + 10 * e), torch.log(10 + e)) # s(x) = log(1+e^x)\n",
    "    return s_x\n",
    "Y = softplus_10(X0)\n",
    "f = torch.log(torch.abs(torch.det(torch.matmul(Y.T, Y))))\n",
    "f.backward()\n",
    "X0.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) 在有监督学习中，如果因变量的取值是非负整数，那么可以使用泊松分布来对因变量进行建模。已知泊松分布 $Y\\sim Poisson(\\lambda)$ 的概率质量函数为 $P(Y=y)=\\lambda^y e^{-\\lambda}/y!$，$y=0,1,\\ldots$。对于每一个因变量观测 $Y_i$，假定它服从均值为 $\\lambda_i>0$ 的泊松分布，其中 $\\lambda_i$ 依赖于自变量 $x_i$：**\n",
    "\n",
    "$$Y_i\\sim Poisson(\\lambda_i),\\quad\\lambda_i=f_\\theta(x_i),$$\n",
    "\n",
    "**这里 $f_\\theta(x)$ 是一个前馈神经网络，$\\theta$ 是网络的所有参数。**\n",
    "\n",
    "**给定自变量矩阵 $X_{n\\times p}$ 和因变量向量 $Y_{n\\times 1}$，我们希望对神经网络的参数进行估计，使其能够尽可能贴合数据。请编写 PyTorch 程序，完成以下任务：1. 创建一个具有两个隐藏层的前馈神经网络用来表达 $f_\\theta(x)$，其中隐藏神经元数量分别为10和3，选择合适的激活函数，尤其是输出层的激活函数（注意 $\\lambda_i$ 的取值范围）；2. 利用极大似然准则推导出该模型的损失函数，进行必要的文字说明；3. 用程序实现损失函数，它以 `x` 和 `y` 作为参数，然后返回当前模型在输入数据上的损失函数取值。调用你编写的函数，计算它在如下模拟数据上的损失函数取值。（20分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss = 17.92801284790039\n",
      "iteration 50, loss = 8.500094413757324\n",
      "iteration 100, loss = 7.08120059967041\n",
      "iteration 150, loss = 6.248605728149414\n",
      "iteration 200, loss = 5.732193946838379\n",
      "iteration 250, loss = 5.383909225463867\n",
      "iteration 300, loss = 5.1292219161987305\n",
      "iteration 350, loss = 4.936450004577637\n",
      "iteration 400, loss = 4.7794508934021\n",
      "iteration 450, loss = 4.647375106811523\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "# 模拟数据\n",
    "torch.manual_seed(2023)\n",
    "X = torch.randn(1000, 30)\n",
    "Y = torch.poisson(10.0 * torch.rand(1000, 1))\n",
    "\n",
    "# 在此处完成代码实现\n",
    "def softplus(x):\n",
    "    # 完成函数体\n",
    "    e = torch.exp(-torch.abs(x)) \n",
    "    sx = torch.where(x >= 0, x + torch.log(1.0 + e), torch.log(1.0 + e))\n",
    "    return sx\n",
    "\n",
    "\n",
    "n = X.shape[0]  # 样本量\n",
    "p = X.shape[1]  # 输入维度\n",
    "d = Y.shape[1]  # 输出维度\n",
    "w1 = torch.randn((p,10),requires_grad = True)\n",
    "b1 = torch.randn((10,1),requires_grad = True)\n",
    "w2 = torch.randn((10,3),requires_grad = True)\n",
    "b2 = torch.randn((3,1),requires_grad = True)\n",
    "w3 = torch.randn((3,d),requires_grad = True)\n",
    "b3 = torch.randn((d,1),requires_grad = True)\n",
    "\n",
    "z1 = torch.matmul(X,w1) + torch.t(b1)\n",
    "a1 = softplus(z1)\n",
    "z2 = torch.matmul(a1,w2) + torch.t(b2)\n",
    "a2 = softplus(z2)\n",
    "z3 = torch.matmul(a2,w3) + torch.t(b3)\n",
    "a3 = softplus(z3)\n",
    "\n",
    "lamda = a3\n",
    "loss = torch.mean(lamda - Y*torch.log(lamda) + torch.log(math.factorial(y)))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处完成代码实现\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # 在此处完成函数实现\n",
    "        self.w1 = nn.Parameter(torch.randn(input_dim, hidden_dim1))\n",
    "        self.b1 = nn.Parameter(torch.rand(hidden_dim1, 1))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.w2 = nn.Parameter(torch.randn(hidden_dim1, hidden_dim2))\n",
    "        self.b2 = nn.Parameter(torch.rand(hidden_dim2, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.w3 = nn.Parameter(torch.randn(hidden_dim2, output_dim))\n",
    "        self.b3 = nn.Parameter(torch.rand(output_dim, 1))\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在此处完成函数实现\n",
    "        z1 = torch.matmul(x,self.w1) + torch.t(self.b1)\n",
    "        a1 = self.relu1(z1)\n",
    "        z2 = torch.matmul(a1,self.w2) + torch.t(self.b2)\n",
    "        a2 = self.relu2(z2)\n",
    "        z3 = torch.matmul(a2,self.w3) + torch.t(self.b3)\n",
    "        a3 = self.softplus(z3)\n",
    "        return a3\n",
    "    \n",
    "n = X.shape[0]\n",
    "p = X.shape[1]\n",
    "r = 10\n",
    "k = 3\n",
    "d = Y.shape[1]\n",
    "model = FeedForward(input_dim=p, hidden_dim1=r, hidden_dim2=k, output_dim=d)\n",
    "\n",
    "nepoch = 500\n",
    "learning_rate = 0.001\n",
    "losses = []\n",
    "yhats=[]\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(nepoch):\n",
    "    a3 = model(X)\n",
    "    yhats.append(a3)\n",
    "    loss = -torch.mean(-a3 + Y * torch.log(a3 + 1e-10) - torch.lgamma(Y + 1))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}, loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题：综合题（35分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化流模型（normalizing flow）是一类特殊的神经网络模型（见阅读材料），其最大的特点是具有可逆性。对于一个 $d$ 维的输入 $x\\in\\mathbb{R}^d$，流模型可以看作是一个 $\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$ 的映射 $f_\\theta$，且 $f_\\theta^{-1}$ 存在，其中 $\\theta$ 是该映射的参数。我们一般希望 $z=f_\\theta(x)$ 和 $x=f_\\theta^{-1}(z)$ 都可高效地进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流模型中一类常见的实现被称为 Real NVP，它按如下的方式定义一个基础的可逆映射 $f_\\theta$：对于 $d$ 维输入 $x=(x_1,\\ldots,x_d)'$，首先固定一个整数 $0<k<d$，然后令 $z=(z_1,\\ldots,z_d)'=f_\\theta(x)$，其中\n",
    "\n",
    "$$\\begin{align*}\n",
    "z_{1:k} & =x_{1:k},\\\\\n",
    "z_{(k+1):d} & =x_{(k+1):d}\\odot\\exp(\\sigma(x_{1:k}))+\\mu(x_{1:k}).\n",
    "\\end{align*}$$\n",
    "\n",
    "换言之，$x$ 的前 $k$ 个元素保持不变，而剩下的元素 $x_{(k+1):d}$ 将乘以一个等长的向量 $v$，再加上一个等长的向量 $u$ 得到 $z_{(k+1):d}$，其中 $u$ 和 $v$ 又是前 $k$ 个元素 $x_{1:k}$ 的函数，$v=\\exp(\\sigma(x_{1:k}))$，$u=\\mu(x_{1:k})$，此处 $\\mu(\\cdot)$ 和 $\\sigma(\\cdot)$ 是两个 $\\mathbb{R}^k\\rightarrow\\mathbb{R}^{d-k}$ 的前馈神经网络。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在实际模型实现中，输入数据是一个 $n\\times d$ 的矩阵 $X$，而 $f_\\theta$ 将对 $X$ 的每一行进行上述变换。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real NVP 还有一个重要的性质，就是其雅各比矩阵的行列式具有简单的形式：\n",
    "\n",
    "$$\\log\\left[\\left|\\det\\left(\\frac{\\partial f_\\theta}{\\partial x}\\right)\\right|\\right]=\\sum_{i=1}^{d-k}\\sigma_i(x_{1:k}),$$\n",
    "\n",
    "其中 $\\sigma_i(x_{1:k})$ 是 $\\sigma(x_{1:k})$ 输出的第 $i$ 个元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) 在以下小问中我们始终让 $k=\\lfloor\\frac{d}{2}\\rfloor$，其中 $\\lfloor x\\rfloor$ 表示不超过 $x$ 的最大整数。请编写一个简单的函数用来计算 $k$。（3分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k(d):\n",
    "    # 在此处完成函数实现\n",
    "    k=floor(d/2)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) $\\mu(\\cdot)$ 和 $\\sigma(\\cdot)$ 具有相同的结构，因此我们可以定义一个统一的类，然后从这个类中生成两个前馈神经网络对象。要求该前馈神经网络的类具有两个隐藏层（不包括输入层和输出层），其神经元数量分别为32和16，使用 ReLU 作为激活函数。完成以下模块的构建。（7分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (relu): ReLU()\n",
      ")\n",
      "FeedForward(\n",
      "  (relu): ReLU()\n",
      ")\n",
      "tensor([[ 0.0000,  0.0000, 29.2863],\n",
      "        [ 0.0000,  0.0000, 24.0931],\n",
      "        [ 0.0000,  0.0000, 26.2255]], grad_fn=<ReluBackward0>)\n",
      "tensor([[ 0.0000, 14.9315,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, 10.4147,  0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d, k):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # 在此处完成函数实现\n",
    "        self.w1 = nn.Parameter(torch.randn(k, 32))\n",
    "        self.b1 = nn.Parameter(torch.rand(32, 1))\n",
    "        self.w2 = nn.Parameter(torch.randn(32, 16))\n",
    "        self.b2 = nn.Parameter(torch.rand(16, 1))\n",
    "        self.w3 = nn.Parameter(torch.randn(16, d-k))\n",
    "        self.b3 = nn.Parameter(torch.rand(d-k, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在此处完成函数实现\n",
    "        z1 = torch.matmul(x,self.w1) + torch.t(self.b1)\n",
    "        a1 = self.relu(z1)\n",
    "        z2 = torch.matmul(a1,self.w2) + torch.t(self.b2)\n",
    "        a2 = self.relu(z2)\n",
    "        z3 = torch.matmul(a2,self.w3) + torch.t(self.b3)\n",
    "        a3 = self.relu(z3)\n",
    "        return a3\n",
    "\n",
    "# 测试结果\n",
    "torch.manual_seed(2023)\n",
    "mu_fn = FeedForward(d=5, k=2)\n",
    "sigma_fn = FeedForward(d=5, k=2)\n",
    "x1k = torch.randn(3, 2)\n",
    "\n",
    "print(mu_fn)\n",
    "print(sigma_fn)\n",
    "print(mu_fn(x1k))\n",
    "print(sigma_fn(x1k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) 利用以上模块，编写一个 Real NVP 的模块类，用来实现标准化流模型。该模块类的 `forward()` 函数接收一个 $n\\times d$ 的参数 `x`，然后返回变换的结果 `z`（$n\\times d$ 的矩阵）以及雅各比矩阵的对数行列式取值 `logdet`（$n\\times 1$ 的向量）。（15分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4305, -0.3499,  0.4749,  0.9041, -0.7021,  1.5963,  0.4228],\n",
      "        [-0.6940,  0.9672,  1.5569, -2.3860,  0.6994, -1.0325, -2.6043],\n",
      "        [ 0.9337, -0.1050,  0.7427, -1.3397, -0.3649, -0.2325,  0.3958],\n",
      "        [ 0.8536, -0.4204, -1.4516,  1.0055, -0.1263, -0.3242, -1.2767],\n",
      "        [ 0.2009,  0.0190,  0.3041, -0.9213,  0.9191, -2.4946, -0.2740]])\n",
      "tensor([[ 4.3048e-01, -3.4990e-01,  4.7494e-01,  8.2897e+00, -5.6389e+00,\n",
      "          2.0119e+01,  1.6767e+00],\n",
      "        [-6.9397e-01,  9.6718e-01,  1.5569e+00, -4.1405e+01,  7.2613e+00,\n",
      "          4.4362e+01,  1.6670e+01],\n",
      "        [ 9.3368e-01, -1.0496e-01,  7.4267e-01, -2.0025e+01, -4.4042e+00,\n",
      "          2.9194e+01,  9.2534e+00],\n",
      "        [ 8.5357e-01, -4.2040e-01, -1.4516e+00,  4.3219e+01, -1.4784e+00,\n",
      "          4.3724e+01,  9.9918e+00],\n",
      "        [ 2.0085e-01,  1.8960e-02,  3.0411e-01, -4.1140e+00,  7.4155e+00,\n",
      "          1.7455e+01,  8.5184e+00]], grad_fn=<CatBackward0>)\n",
      "tensor([22.9705, 61.0318, 39.2541, 63.5787, 29.5714], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RealNVP, self).__init__()\n",
    "        # 在此处完成函数实现\n",
    "        self.k = get_k(input_dim)\n",
    "        self.mu = FeedForward(d=input_dim, k=self.k)\n",
    "        self.sigma = FeedForward(d=input_dim, k=self.k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在此处完成函数实现\n",
    "        x1, x2 = x[:, :self.k], x[:, self.k:]\n",
    "        z1 = x1\n",
    "        z2 = x2 * self.mu(x1) + self.sigma(x1)\n",
    "        z = torch.cat([z1, z2], 1)\n",
    "        \n",
    "        log_jacob = torch.sum(self.sigma(x1),dim = 1)\n",
    "        return z, log_jacob\n",
    "\n",
    "# 测试结果\n",
    "torch.manual_seed(2023)\n",
    "n = 5\n",
    "d = 7\n",
    "x = torch.randn(n, d)\n",
    "model = RealNVP(input_dim=d)\n",
    "z, logdet = model(x)\n",
    "\n",
    "print(x)\n",
    "print(z)\n",
    "print(logdet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) 根据阅读材料中的介绍，我们用一个流模型 $f_\\theta$ 来表达一个 $d$ 维的密度函数，其中 $p_Z$ 是 $d$ 维标准正态分布的密度函数：**\n",
    "\n",
    "$$\n",
    "\\log[p_{X}(x)]=\\log\\left[\\left|\\det\\left(\\frac{\\partial f_\\theta}{\\partial x}\\right)\\right|\\right]+\\log[p_{Z}(f_\\theta(x))].\n",
    "$$\n",
    "\n",
    "**请利用上面建立的 Real NVP 模型，计算该分布模型在 (c) 中数据 `x` 上的对数似然函数值。（5分）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此处完成代码实现\n",
    "import torch.distributions as D\n",
    "norm = D.MultivariateNormal(torch.zeros(d), torch.eye(d))\n",
    "log_x = logdet + norm.log_prob(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -237.7831, -1953.8263,  -647.0463, -1885.1901,  -201.4973],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) 在上面定义的 Real NVP 模型中，$f_\\theta$ 的变换输出 $z$ 的前 $k$ 个分量与输入 $x$ 的前 $k$ 个分量是完全一致的，这会使得模型的表达能力受到限制。结合阅读材料，请用文字简述有哪些方法可以增强模型的表达能力，使其能刻画更复杂的非线性关系。（5分）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加网络深度： 增加 Real NVP 模型的网络深度可以提高其表达能力。通过增加隐藏层的数量和每个隐藏层的神经元数量，模型可以学习更复杂的非线性映射。这可以通过在定义 Real NVP 模型时增加隐藏层的数量和神经元数量来实现。\n",
    "\n",
    "使用更复杂的网络结构： 使用更复杂的神经网络结构，如卷积神经网络（CNN）或循环神经网络（RNN），可以增加模型的表达能力，特别是在处理图像、序列等复杂数据结构时。\n",
    "\n",
    "引入自注意力机制： 自注意力机制（self-attention mechanism）可以使模型在学习长程依赖性时更加有效。Transformer 模型中的自注意力机制已被证明在处理序列数据时非常成功，可以考虑在 Real NVP 模型中引入类似的机制。\n",
    "\n",
    "使用更复杂的分布变换： Real NVP 模型中使用的分布变换是仿射变换，可以考虑使用更复杂的变换，例如非线性的深度变换。这可以通过修改 Real NVP 模型中的变换层来实现，以使其更适应数据的复杂结构。\n",
    "\n",
    "考虑使用其他生成模型： 除了 Real NVP，还有其他生成模型，如变分自动编码器（Variational Autoencoder，VAE）和生成对抗网络（Generative Adversarial Network，GAN）。这些模型可能在不同类型的数据和任务上表现更好，具有更强大的表达能力。\n",
    "\n",
    "总体而言，提高模型表达能力的方法包括增加网络深度、使用更复杂的网络结构、引入自注意力机制、使用更复杂的分布变换以及考虑其他生成模型。选择合适的方法取决于具体的任务和数据特性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
