{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch基础+线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch 基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如遇到不清楚的函数或主题，可以查阅[官方文档](https://pytorch.org/docs/stable/index.html)或利用搜索引擎寻求帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 必备代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 通常会与 Numpy 包共同使用，因此导入 PyTorch 的时候按惯例也把 Numpy 加载进来。更重要的是，在**每个** PyTorch 程序的最开头都应设置好随机数种子，从而让结果可重复。**作业中也应在每题的代码开头设置随机数种子**。由于 PyTorch 和 Numpy 使用了两套随机数生成机制，因此还需分别设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1dcb933dbf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 中最为重要的数据结构是张量（Tensor），可以简单理解为多维数组，我们一般使用的向量和矩阵都是张量的特例。本次作业中我们只用到一维和二维 Tensor，分别对应向量和矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor 可以通过 `torch.tensor()` 加上给定的数据创建，注意矩阵是按行创建的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 5.])\n"
     ]
    }
   ],
   "source": [
    "vec = torch.tensor([1.0, 2.0, 5.0])\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 2.0000, 2.0000],\n",
      "        [3.0000, 5.0000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "mat = torch.tensor([[1.0, 2.0, 2.0], [3.0, 5.0, 4.5]])\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些常用的 Tensor 有专用的创建方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(3, 10, steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以给定形状生成随机数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8645,  0.4071, -1.1971],\n",
       "        [ 0.3489, -1.1437, -0.6611]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor 的形状可以通过 `shape` 属性来获取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(vec.shape)\n",
    "print(mat.shape)\n",
    "n = mat.shape[0]\n",
    "p = mat.shape[1]\n",
    "print(n)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 Tensor 变形："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [5.]])\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [2.0000, 3.0000],\n",
      "        [5.0000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(vec.view(3, 1))\n",
    "print(mat.view(3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 矩阵向量运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor 可以很直观地与标量进行运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4., 7.])\n",
      "tensor([[1.2000, 2.4000, 2.4000],\n",
      "        [3.6000, 6.0000, 5.4000]])\n"
     ]
    }
   ],
   "source": [
    "print(vec + 2.0)\n",
    "print(mat * 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逐元素运算的数学函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2.7183,   7.3891, 148.4132])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8415,  0.9093,  0.9093],\n",
       "        [ 0.1411, -0.9589, -0.9775]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sin(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "汇总运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6667)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.5000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按坐标轴汇总："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.0000, 7.0000, 6.5000])\n",
      "tensor([ 5.0000, 12.5000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(mat, dim=0))\n",
    "print(torch.sum(mat, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵乘法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15.0000, 35.5000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(mat, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭配转置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.0000, 17.0000, 15.5000],\n",
       "        [17.0000, 29.0000, 26.5000],\n",
       "        [15.5000, 26.5000, 24.2500]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.t(mat), mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 统计分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 包含了许多常见的统计分布，参见[说明文档](https://pytorch.org/docs/stable/distributions.html)。大部分分布都可以计算 p.d.f. 和 c.d.f. 等函数，以及进行随机模拟抽样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一个正态分布 $N(1, 3)$ 对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as D\n",
    "import math\n",
    "\n",
    "norm = D.Normal(loc=torch.tensor([1.0]), scale=torch.tensor([math.sqrt(3.0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 $x = 1,2,3$ 处的对数密度函数值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4682, -1.6349, -2.1349])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.log_prob(torch.tensor([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成5个随机数（注意这里形状的写法）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9078],\n",
       "        [ 2.6222],\n",
       "        [ 1.6566],\n",
       "        [ 1.5657],\n",
       "        [ 2.8333]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.sample(sample_shape=(5,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分布的参数可以是一个向量，例如三个分别以0.1, 0.5和0.9为参数的 Bernoulli 分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern = D.Bernoulli(probs=torch.tensor([0.1, 0.5, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各自生成一个随机数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bern.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他常见的操作可以参考[官方教程](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)，完整的函数列表可以查看[官方 API 文档](https://pytorch.org/docs/stable/torch.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 练习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 生成一个大小为 $[n \\times p] = [200 \\times 10]$ 的数据矩阵 `x`，用正态分布 N(0, 2) 填充。随机数种子设为123456。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "p = 10\n",
    "\n",
    "# 完成后续的代码\n",
    "x = torch.randn(n, p) * 2 # 替换这里的代码\n",
    "\n",
    "# 检查 x 的大小，方便 debug\n",
    "assert x.shape == (n, p), \"x 形状有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 生成一个长度为 $p$ 的向量 `beta`，每个元素服从均匀分布 Uniform(-1, 1)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.rand(p) * 2 - 1  # 替换这里的代码\n",
    "\n",
    "# 检查 beta 的长度，方便 debug\n",
    "assert beta.shape == (p,), \"beta 长度有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 生成一个长度为 $n$ 的向量 `eps`，每个元素服从独立正态 $N(0, 0.1)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = torch.randn(n) * 0.1  # 替换这里的代码\n",
    "\n",
    "# 检查 eps 的长度，方便 debug\n",
    "assert eps.shape == (n,), \"eps 长度有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 创建向量 `y`，令其在数学上等于 $y=X\\beta+\\epsilon$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.matmul(x, beta) + eps  # 替换这里的代码\n",
    "\n",
    "# 检查 y 的长度，方便 debug\n",
    "assert y.shape == (n,), \"y 长度有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) 回归问题：给定数据 `x` 和 `y`，估计 `beta` 的取值。以 MSE 为损失函数，编写 Python 函数 `loss_fn_reg(bhat, x, y)`，用来返回任意 $\\hat{\\beta}$ 下的目标函数值。请用基础的矩阵和向量运算实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_reg(bhat, x, y):\n",
    "    # 编写函数主体，替换这里的代码\n",
    "    y_pred = torch.matmul(x, bhat)\n",
    "    residuals = y - y_pred\n",
    "    mse = torch.mean(residuals**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Pytorch 中也提供了 MSE 损失函数，参见[其文档](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)。其用法是先建立一个损失函数对象，然后将 $\\hat{y}$ 和 $y$ 作为参数传入。请利用这种方法计算如下给定 $\\hat{\\beta}$ 后的损失函数值，并与你自己的函数结果进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(46.3286)\n",
      "tensor(46.3286)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "bhat = torch.ones(p)\n",
    "yhat = torch.matmul(x, bhat)  # 替换这里的代码计算 yhat\n",
    "\n",
    "mse_reg = nn.MSELoss()\n",
    "loss1 = mse_reg(yhat, y)\n",
    "\n",
    "loss2 = loss_fn_reg(bhat, x, y)\n",
    "\n",
    "print(loss1)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得了一致的计算结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 与第1题类似创建变量 `x` 和 `beta`，但使用不同的 `n` 和 `p`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)\n",
    "n = 150\n",
    "p = 6\n",
    "x = torch.randn(n, p) * 2  # 替换这里的代码\n",
    "beta = torch.rand(p) * 2 - 1  # 替换这里的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 定义函数 `sigmoid(x)`，其中 `x` 是一个 Tensor，$\\mathrm{sigmoid}(x)=e^x/(1+e^x)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # 编写函数主体，替换这里的代码\n",
    "    e = torch.exp(-torch.abs(x))\n",
    "    numer = torch.where(x>=0, 1.0, e)\n",
    "    denom = 1.0 + e\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 根据分布 $Y|X\\sim Bernoulli(\\mathrm{sigmoid}(X\\beta))$，生成 $Y$ 的随机数。提示：参照1.4节的方法，先计算 Bernoulli 分布的参数**向量**，然后生成随机数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_7124\\4183597191.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bern = D.Bernoulli(probs=torch.tensor(rho))\n"
     ]
    }
   ],
   "source": [
    "rho = sigmoid(torch.matmul(x, beta))\n",
    "bern = D.Bernoulli(probs=torch.tensor(rho))  \n",
    "y = bern.sample() # 替换这里的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 已知 $Bernoulli(\\rho)$ 分布的对数密度函数为 $\\log p(y;\\rho)=y\\log \\rho + (1-y) \\cdot \\log(1-\\rho)$。根据此信息，推导出给定 $\\hat{\\beta}$ 时的对数似然函数，并编写损失函数 `loss_fn_logistic(bhat, x, y)`，返回**负**对数似然值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y|x,\\hat{\\beta } ~\\sim Bernoulli(\\rho (\\hat{\\beta }'x))$\n",
    "\n",
    "$\\rho(\\hat{\\beta }'x)=1/(1+e^{-\\hat{\\beta }'x})$\n",
    "\n",
    "$\\log p(y;x,\\hat{\\beta })=y\\log \\rho(\\hat{\\beta }'x) + (1-y) \\cdot \\log(1-\\rho(\\hat{\\beta }'x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_logistic(bhat, x, y):\n",
    "    # 编写函数主体，替换这里的代码\n",
    "    rho_x = torch.matmul(x, bhat) # rho(x) = XB\n",
    "    e = torch.exp(-torch.abs(rho_x)) # e^(-x)\n",
    "    log1e = torch.log(1 + e) # log(1+e)\n",
    "    s_x = torch.where(rho_x >= 0, rho_x + log1e, log1e) # s(x) = log(1+e^x)\n",
    "    loss = -torch.mean(y * (rho_x - s_x) + (1 - y) * (-s_x))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Pytorch 中也提供了 BCELoss 损失函数，参见[其文档](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)。其用法是先建立一个损失函数对象，然后将 $\\hat{\\rho}$ 和 $y$ 作为参数传入。请利用这种方法计算如下给定 $\\hat{\\beta}$ 后的损失函数值，并与你自己的函数结果进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7017)\n",
      "tensor(1.7017)\n"
     ]
    }
   ],
   "source": [
    "bhat = torch.ones(p)\n",
    "rhohat = sigmoid(torch.matmul(x, bhat))  # 替换这里的代码计算 rhohat = sigmoid(x * betahat)\n",
    "\n",
    "bce_logistic = nn.BCELoss()\n",
    "loss1 = bce_logistic(rhohat, y)\n",
    "\n",
    "loss2 = loss_fn_logistic(bhat, x, y)\n",
    "\n",
    "print(loss1)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得了一致的计算结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 多分类问题的数据通常包括数据阵 $X$ 和标签向量 $l$，其中标签为整数。在计算损失函数时，我们需要先将 $l$ 转换成多项分布的0-1数据，即所谓 One-hot 编码。运行并观察下面的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 2, 1, 0, 3, 3, 3, 3, 0, 3, 0, 0, 2, 2, 0, 3, 0, 3, 3])\n",
      "torch.Size([200, 4])\n",
      "tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123456)\n",
    "torch.manual_seed(123456)\n",
    "n = 200  # 样本量\n",
    "p = 10   # 变量数\n",
    "k = 4    # 类别数\n",
    "x = torch.randn(n, p)\n",
    "l = torch.tensor(np.random.choice(range(4), size=n, replace=True), dtype=int)\n",
    "print(l[:20])\n",
    "\n",
    "y = torch.nn.functional.one_hot(l)\n",
    "print(y.shape)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 创建矩阵 `W`，大小为 $k \\times p$，用 N(0, 1) 填充其取值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(k, p)  # 替换这里的代码\n",
    "\n",
    "# 检查 w 的形状，方便 debug\n",
    "assert w.shape == (k, p), \"w 形状有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 接下来计算对 $Y$ 的概率预测值，其中每个 $Y_i$ 观测对应一个等长的概率向量 $p_i$，而 $p_i=\\mathrm{Softmax}(Wx_i)$。首先计算 $Wx_i$，其中 $x_i$ 是第 $i$ 个观测。由于 $X$ 是把 $x_i$ 按行组合，因此矩阵形式表达为 $U=XW'$，其中 $U$ 的第 $i$ 行即为 $Wx_i$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.matmul(x, torch.t(w))  # 替换这里的代码\n",
    "\n",
    "# 检查 u 的形状，方便 debug\n",
    "assert u.shape == (n, k), \"u 形状有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先测试一下 $\\mathrm{Softmax}(Wx_{100})$ 的结果，观察其元素和是否为1。代码中的 `dim=0` 意思是对第一个下标方向计算 Softmax，由于 `u[99]` 是一个向量，因此第一个下标方向就是该向量本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1958, 0.0477, 0.6024, 0.1541])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(u[99], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而为了对 $U$ 的每一行都计算 Softmax，我们可以直接对整个 `u` 矩阵用 `torch.softmax`，其中 `dim` 需指定为1，意思是对第二个下标方向求 Softmax，即对 $U$ 的每一行。原理类似于1.3节的按坐标轴汇总。请完成该计算，得到矩阵 $P$，其中 $P$ 的第 $i$ 行即为 $p_i$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.softmax(u, dim=1)  # 替换这里的代码\n",
    "\n",
    "# 检查 p 的形状，方便 debug\n",
    "assert p.shape == (n, k), \"p 形状有误\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 根据 `y` 和 `p` 两个矩阵，即可根据公式得到对数似然函数值。总结上述步骤，编写损失函数 `loss_fn_softmax(w, x, y)`，返回**负**对数似然值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_softmax(w, x, y):\n",
    "    # 编写函数主体，替换这里的代码\n",
    "    u = torch.matmul(x, torch.t(w))\n",
    "    p = torch.softmax(u, dim=1)\n",
    "    loss = -torch.mean(torch.sum(y * torch.log(p), dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Pytorch 中也提供了 CrossEntropyLoss 损失函数，参见[其文档](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)。其用法是先建立一个损失函数对象，然后将 $U$ 和 $l$ 作为参数传入（注意 $U$ 是经过 Softmax **之前**的矩阵，$l$ 是**原始**的标签）。请利用这种方法计算损失函数值，并与你自己的函数结果进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3032)\n",
      "tensor(3.3032)\n"
     ]
    }
   ],
   "source": [
    "ce_softmax = nn.CrossEntropyLoss()\n",
    "loss1 = ce_softmax(u, l)\n",
    "\n",
    "loss2 = loss_fn_softmax(w, x, y)\n",
    "\n",
    "print(loss1)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得了一致的计算结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
